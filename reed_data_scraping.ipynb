{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad33c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCRAPING DATA ##\n",
    "\n",
    "# Compiling list of directory pages \n",
    "\n",
    "url = requests.get('https://www.reed.co.uk/recruiterdirectory')\n",
    "soup = bs(url.text, 'html.parser')\n",
    "\n",
    "pages = []\n",
    "\n",
    "for item in soup.find_all('li', attrs={'class':'page'}):\n",
    "    pages.append(item.text)\n",
    "    \n",
    "final_page = int(items[len(items)-2])\n",
    "\n",
    "numbers = []\n",
    "for i in range(1, final_page):\n",
    "    numbers.append(str(i))\n",
    "    \n",
    "page_urls = []\n",
    "\n",
    "for i in range(0, (final_page-1)):\n",
    "    base_url = 'http://reed.co.uk/recruiterdirectory?pageno='\n",
    "    page_url = base_url + numbers[i]\n",
    "    page_urls.append(page_url)\n",
    "\n",
    "# Compiling list of employer page urls \n",
    "\n",
    "employer_pages = []    \n",
    "\n",
    "for page in page_urls:\n",
    "    \n",
    "    url = requests.get(page)\n",
    "    soup = bs(url.text, 'html.parser')\n",
    "\n",
    "    for elm in soup.find_all('div', attrs={'class':'subtitle'}):\n",
    "        for a_elm in elm.find_all('a'):\n",
    "            link = a_elm.attrs['href']\n",
    "            employer_page = 'https://www.reed.co.uk' + link\n",
    "            employer_pages.append(employer_page)\n",
    "            print(employer_page)\n",
    "            \n",
    "    sleep(randint(0, 5))\n",
    "    \n",
    "# Compiling list of employer profiles urls \n",
    "    \n",
    "employer_profiles_raw = []\n",
    "\n",
    "for i in range(0, len(employer_pages)):\n",
    "    \n",
    "    url = requests.get(employer_pages[i])\n",
    "    soup = bs(url.text, 'html.parser')\n",
    "    profile = soup.find('a', href=re.compile('employer-profile'))\n",
    "    employer_profiles_raw.append(profile)\n",
    "    sleep(random.uniform(5, 15))\n",
    "\n",
    "employer_profiles = []\n",
    "\n",
    "for profile_raw in employer_profiles_raw:\n",
    "    if profile_raw != None:\n",
    "        profile_string = str(profile_raw)\n",
    "        search_str = re.search(''(.+?)'', profile_string)\n",
    "        link_suf = search_str.group(1)\n",
    "        full_link = 'https://www.reed.co.uk' + link_suf\n",
    "        employer_profiles.append(full_link)\n",
    "        print(full_link)\n",
    "    else:\n",
    "        employer_profiles.append('No profile')\n",
    "        print('No profile')\n",
    "        \n",
    "# Exporting employer pages and profiles to df \n",
    "\n",
    "pages_profiles = pd.DataFrame(list(zip(employer_pages, employer_profiles)), columns=['employer_page', 'employer_profile'])\n",
    "pages_profiles.to_csv('/Users/elenaleggett/Documents/Masters/Dissertation/gender_pay_gap/data_scripts/data/pages_profiles.csv')\n",
    "\n",
    "# Removing companies with no profiles\n",
    "\n",
    "profile_pages = []\n",
    "\n",
    "for profile in employer_profiles:\n",
    "    if profile != 'No profile':\n",
    "        profile_pages.append(profile)\n",
    "\n",
    "jobs_pages = []\n",
    "\n",
    "for i in range(0, len(employer_pages)):\n",
    "    if employer_profiles[i] != 'No profile':\n",
    "        jobs_pages.append(employer_pages[i]) \n",
    "        \n",
    "# Compiling profile page variables \n",
    "\n",
    "names = []\n",
    "sizes = []\n",
    "sectors = []\n",
    "what_we_dos = []\n",
    "what_youll_gets = []\n",
    "who_youll_work_withs = []\n",
    "\n",
    "for i in range(0, len(profile_pages)):\n",
    "    \n",
    "    url = requests.get(profile_pages[i])\n",
    "    soup = bs(url.text, 'html.parser')\n",
    "    \n",
    "    # Name\n",
    "    \n",
    "    title_raw = soup.find('span', {'class':'title'})\n",
    "    title = title_raw.string\n",
    "    name = title[11:]\n",
    "    names.append(name.lower())\n",
    "    \n",
    "    # Size \n",
    "    \n",
    "    size_raw = soup.find('span', {'class':'information__description'})\n",
    "    size = size_raw.string\n",
    "    sizes.append(size)\n",
    "    \n",
    "    # Sector\n",
    "    \n",
    "    sector_raw = size_raw.find_next('span', {'class':'information__description'})\n",
    "    sector = sector_raw.string\n",
    "    sectors.append(sector)\n",
    "    \n",
    "    # What we do \n",
    "    \n",
    "    do_heading = soup.find(text = re.compile('What we do'), attrs = {'class':'heading'})\n",
    "\n",
    "    if do_heading != None:\n",
    "        do_text = do_heading.find_next_sibling('div')\n",
    "        do_str = str(do_text)\n",
    "        a = re.compile('<.*?>')\n",
    "        do_str = a.sub(' ', do_str)\n",
    "        do_str = do_str.replace('\\n', '')\n",
    "        do_str = do_str.replace('\\xa0', '')\n",
    "    else: \n",
    "        do_str = 'Empty'\n",
    "        \n",
    "    what_we_dos.append(do_str)\n",
    "        \n",
    "    # What you'll get \n",
    "    \n",
    "    get_heading = soup.find(text = re.compile('What you'll get'), attrs = {'class':'heading'})\n",
    "\n",
    "    if get_heading != None:\n",
    "        get_text = get_heading.find_next_sibling('ul')\n",
    "        get_str = str(get_text.get_text())\n",
    "        get_str = get_str.replace('\\n\\n\\n\\n', '; ')\n",
    "        get_str = get_str.replace('\\n', '')\n",
    "\n",
    "    else:\n",
    "        get_str = 'Empty'\n",
    "    \n",
    "    what_youll_gets.append(get_str)\n",
    "    \n",
    "    # Who you'll work with\n",
    "    \n",
    "    who_heading = soup.find(text = re.compile('Who you'll work with'), attrs = {'class':'heading'})\n",
    "\n",
    "    if who_heading != None:\n",
    "\n",
    "        who_text = who_heading.find_next_sibling('div')\n",
    "        who_str = str(who_text)\n",
    "        a = re.compile('<.*?>')\n",
    "        who_str = a.sub(' ', who_str)    \n",
    "        who_str = who_str.replace('\\n', '')\n",
    "        who_str = who_str.replace('\\xa0', '')\n",
    "\n",
    "    else:\n",
    "        who_str = 'Empty'\n",
    "        \n",
    "    who_youll_work_withs.append(who_str)\n",
    "    \n",
    "    sleep(random.uniform(5, 15))\n",
    "    \n",
    "# Jobs page variables \n",
    "\n",
    "ft_jobs = []\n",
    "pt_jobs = []\n",
    "\n",
    "for i in range(0, len(jobs_pages)):\n",
    "\n",
    "    url = requests.get(jobs_pages[i])\n",
    "    soup = bs(url.text, 'html.parser')\n",
    "    \n",
    "    # FT jobs\n",
    "    \n",
    "    ft_raw = soup.find('li', {'data-jobtype':'FullTime'})\n",
    "    ft_count = ft_raw.find('span', {'class':'count'})\n",
    "    ft_text = ft_count.get_text()\n",
    "    ft_text = ft_text.replace('(', '')\n",
    "    ft_text = ft_text.replace(')', '')\n",
    "    ft_text = ft_text.replace(',', '')\n",
    "    ft_jobs.append(ft_text)\n",
    "    \n",
    "    # PT jobs\n",
    "    \n",
    "    pt_raw = soup.find('li', {'data-jobtype':'PartTime'})\n",
    "    pt_count = pt_raw.find('span', {'class':'count'})\n",
    "    pt_text = pt_count.get_text()\n",
    "    pt_text = pt_text.replace('(', '')\n",
    "    pt_text = pt_text.replace(')', '')\n",
    "    pt_text = pt_text.replace(',', '')\n",
    "    pt_jobs.append(pt_text)\n",
    "    \n",
    "    sleep(random.uniform(5, 15))\n",
    "    \n",
    "\n",
    "## PROCESSING DATA ##\n",
    "    \n",
    "    \n",
    "# Processing text columns \n",
    "\n",
    "what_we_dos_edited = []\n",
    "\n",
    "for element in what_we_dos:\n",
    "    element_lower = element.lower()\n",
    "    what_we_dos_edited.append(element_lower)\n",
    "    \n",
    "what_we_dos_final = []\n",
    "\n",
    "for element in what_we_dos_edited:\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    element = regex.sub('', element)\n",
    "    element = ' '.join([word for word in element.split() if word not in stopwords.words()])\n",
    "    what_we_dos_final.append(element)\n",
    "    \n",
    "what_youll_gets_edited = []\n",
    "\n",
    "for element in what_youll_gets:\n",
    "    element_lower = element.lower()\n",
    "    what_youll_gets_edited.append(element_lower)\n",
    "    \n",
    "what_youll_gets_final = []\n",
    "\n",
    "for element in what_youll_gets_edited:\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    element = regex.sub('', element)\n",
    "    element = ' '.join([word for word in element.split() if word not in stopwords.words()])\n",
    "    what_youll_gets_final.append(element)\n",
    "    \n",
    "who_youll_work_withs_edited = []\n",
    "\n",
    "for element in who_youll_work_withs:\n",
    "    element_lower = element.lower()\n",
    "    who_youll_work_withs_edited.append(element_lower)\n",
    "    \n",
    "who_youll_work_withs_final = []\n",
    "\n",
    "for element in who_youll_work_withs_edited:\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    element = regex.sub('', element)\n",
    "    element = ' '.join([word for word in element.split() if word not in stopwords.words()])\n",
    "    who_youll_work_withs_final.append(element)\n",
    "    \n",
    "agentic = ['control ', 'controls ', 'controlling ', 'controller ', 'initiative ', 'initiatives ', 'motivation ', 'motivations ', 'motivated ', 'pressure ', 'pressured ', 'pressures ', 'productive ', 'productivity ', 'responsible ', 'responsibilities ', 'power ', 'powerful ', 'competitive ', 'driven ', 'drive ']\n",
    "communal = ['punctual', 'punctuality', 'honest', 'honesty', 'attentive', 'attentiveness', 'teamwork', 'helpful', 'helpfulness', 'courteous', 'courtesy', 'courteousness']\n",
    "inclusive = ['diversity ', 'diverse ', 'inclusion ', 'inclusive ', 'family friendly ', 'parental ', 'flexible ', 'flexibility ', 'collective ', 'collectivity ', 'equal ', 'equality ', 'share ', 'sharing ']\n",
    "\n",
    "agentic_counts_wwd = []\n",
    "\n",
    "agentic_count = 0\n",
    "\n",
    "for description in what_we_dos_final:\n",
    "    agentic_count = 0\n",
    "    for word in agentic:\n",
    "        if word in description:\n",
    "            agentic_count = agentic_count + 1\n",
    "        else:\n",
    "            agentic_count = agentic_count + 0 \n",
    "    agentic_counts_wwd.append(agentic_count)\n",
    "    \n",
    "communal_counts_wwd = []\n",
    "\n",
    "communal_count = 0\n",
    "\n",
    "for description in what_we_dos_final:\n",
    "    communal_count = 0\n",
    "    for word in communal:\n",
    "        if word in description:\n",
    "            communal_count = communal_count + 1\n",
    "        else:\n",
    "            communal_count = communal_count + 0 \n",
    "    communal_counts_wwd.append(communal_count)\n",
    "    \n",
    "inclusive_counts_wwd = []\n",
    "\n",
    "inclusive_count = 0\n",
    "\n",
    "for description in what_we_dos_final:\n",
    "    inclusive_count = 0\n",
    "    for word in inclusive:\n",
    "        if word in description:\n",
    "            inclusive_count = inclusive_count + 1\n",
    "        else:\n",
    "            inclusive_count = inclusive_count + 0 \n",
    "    inclusive_counts_wwd.append(inclusive_count)\n",
    "    \n",
    "agentic_counts_wyg = []\n",
    "\n",
    "agentic_count = 0\n",
    "\n",
    "for description in what_youll_gets_final:\n",
    "    agentic_count = 0\n",
    "    for word in agentic:\n",
    "        if word in description:\n",
    "            agentic_count = agentic_count + 1\n",
    "        else:\n",
    "            agentic_count = agentic_count + 0 \n",
    "    agentic_counts_wyg.append(agentic_count)\n",
    "    \n",
    "communal_counts_wyg = []\n",
    "\n",
    "communal_count = 0\n",
    "\n",
    "for description in what_youll_gets_final:\n",
    "    communal_count = 0\n",
    "    for word in communal:\n",
    "        if word in description:\n",
    "            communal_count = communal_count + 1\n",
    "        else:\n",
    "            communal_count = communal_count + 0 \n",
    "    communal_counts_wyg.append(communal_count)\n",
    "    \n",
    "inclusive_counts_wyg = []\n",
    "\n",
    "inclusive_count = 0\n",
    "\n",
    "for description in what_youll_gets_final:\n",
    "    inclusive_count = 0\n",
    "    for word in inclusive:\n",
    "        if word in description:\n",
    "            inclusive_count = inclusive_count + 1\n",
    "        else:\n",
    "            inclusive_count = inclusive_count + 0 \n",
    "    inclusive_counts_wyg.append(inclusive_count)\n",
    "    \n",
    "agentic_counts_wyww = []\n",
    "\n",
    "agentic_count = 0\n",
    "\n",
    "for description in who_youll_work_withs_final:\n",
    "    agentic_count = 0\n",
    "    for word in agentic:\n",
    "        if word in description:\n",
    "            agentic_count = agentic_count + 1\n",
    "        else:\n",
    "            agentic_count = agentic_count + 0 \n",
    "    agentic_counts_wyww.append(agentic_count)\n",
    "    \n",
    "communal_counts_wyww = []\n",
    "\n",
    "communal_count = 0\n",
    "\n",
    "for description in who_youll_work_withs_final:\n",
    "    communal_count = 0\n",
    "    for word in communal:\n",
    "        if word in description:\n",
    "            communal_count = communal_count + 1\n",
    "        else:\n",
    "            communal_count = communal_count + 0 \n",
    "    communal_counts_wyww.append(communal_count)\n",
    "    \n",
    "inclusive_counts_wyww = []\n",
    "\n",
    "inclusive_count = 0\n",
    "\n",
    "for description in who_youll_work_withs_final:\n",
    "    inclusive_count = 0\n",
    "    for word in inclusive:\n",
    "        if word in description:\n",
    "            inclusive_count = inclusive_count + 1\n",
    "        else:\n",
    "            inclusive_count = inclusive_count + 0 \n",
    "    inclusive_counts_wyww.append(inclusive_count)\n",
    "    \n",
    "# % part time \n",
    "\n",
    "prop_pt = []\n",
    "\n",
    "ft_nums = []\n",
    "pt_nums = []\n",
    "\n",
    "for i in range(0, len(ft_jobs)):\n",
    "    ft_num = int(ft_jobs[i])\n",
    "    ft_nums.append(ft_num)\n",
    "    pt_num = int(pt_jobs[i])\n",
    "    pt_nums.append(pt_num)\n",
    "               \n",
    "for i in range(0, len(ft_jobs)):\n",
    "    if pt_nums[i] != 0:\n",
    "        if ft_nums[i] != 0:\n",
    "            prop_pt_element = pt_nums[i] / ft_nums[i]\n",
    "            prop_pt_element = prop_pt_element * 100\n",
    "            prop_pt_element = round(prop_pt_element, 3)\n",
    "            prop_pt.append(prop_pt_element)\n",
    "        else:\n",
    "            prop_pt.append(100)\n",
    "    else:\n",
    "        prop_pt.append(0)\n",
    "        \n",
    "\n",
    "# Converting categorical sizes column into numerical\n",
    "\n",
    "sizes_modified = []\n",
    "\n",
    "for i in range(0, len(sizes)):\n",
    "    if sizes[i] == '1–4 employees':\n",
    "        sizes_modified.append(1)\n",
    "    if sizes[i] == '5–9 employees':\n",
    "        sizes_modified.append(5)\n",
    "    if sizes[i] == '10–19 employees':\n",
    "        sizes_modified.append(10)\n",
    "    if sizes[i] == '20–49 employees':\n",
    "        sizes_modified.append(20)\n",
    "    if sizes[i] == '50–99 employees':\n",
    "        sizes_modified.append(50)\n",
    "    if sizes[i] == '100–249 employees':\n",
    "        sizes_modified.append(100)\n",
    "    if sizes[i] == '250–499 employees':\n",
    "        sizes_modified.append(250)\n",
    "    if sizes[i] == '500–999 employees':\n",
    "        sizes_modified.append(500)\n",
    "    if sizes[i] == '1,000–2,499 employees':\n",
    "        sizes_modified.append(1000)\n",
    "    if sizes[i] == '2,500–4,999 employees':\n",
    "        sizes_modified.append(2500)\n",
    "    if sizes[i] == '5,000+ employees':\n",
    "        sizes_modified.append(5000)\n",
    "        \n",
    "# Cleaning name for matching \n",
    "\n",
    "names_lower = []\n",
    "for name in names:\n",
    "    name_lower = name.lower()\n",
    "    names_lower.append(name_lower)\n",
    "    \n",
    "names_cleaned_0 = []\n",
    "for name in names_lower:\n",
    "    if ' ltd' in name:\n",
    "        newname = name.replace(' ltd', ' limited')\n",
    "        names_cleaned_0.append(newname)\n",
    "    else:\n",
    "        names_cleaned_0.append(name)\n",
    "\n",
    "names_cleaned = []\n",
    "for name in names_cleaned_0:\n",
    "    if ' limited' in name:\n",
    "        newname = name.replace(' limited', '')\n",
    "        names_cleaned.append(newname)\n",
    "    else:\n",
    "        names_cleaned.append(name)\n",
    "        \n",
    "# Forming Reed data frame and exporting to csv \n",
    "\n",
    "columns = ['name', 'size', 'sector', 'prop_pt', 'what_we_do_agentic', 'what_we_do_communal', 'what_we_do_inclusive', 'what_youll_get_agentic', 'what_youll_get_communal', 'what_we_do_inclusive', 'who_youll_work_with_agentic', 'who_youll_work_with_communal', 'who_youll_work_with_inclusive']\n",
    "reed_data = pd.DataFrame(list(zip(names_cleaned, sizes_modified, prop_pt, agentic_counts_wwd, communal_counts_wwd, inclusive_counts_wwd, agentic_counts_wyg, communal_counts_wyg, inclusive_counts_wyg, agentic_counts_wyww, communal_counts_wyww, inclusive_counts_wyww)), columns = columns)\n",
    "reed_data.to_csv('reed.data.csv')\n",
    "                         \n",
    "# Editing name in UK gpg dataset \n",
    "                         \n",
    "uk_gpg_all = pd.read_csv('https://gender-pay-gap.service.gov.uk/viewing/download-data/2022')    \n",
    "                         \n",
    "gpg_names = list(uk_gpg_data['EmployerName'])\n",
    "gpg_names_lower = []\n",
    "for name in gpg_names:\n",
    "    name_lower = name.lower()\n",
    "    gpg_names_lower.append(name_lower)\n",
    "    \n",
    "gpg_names_cleaned_0 = []\n",
    "for name in gpg_names_lower:\n",
    "    if ' ltd' in name:\n",
    "        newname = name.replace(' ltd', ' limited')\n",
    "        gpg_names_cleaned_0.append(newname)\n",
    "    else:\n",
    "        gpg_names_cleaned_0.append(name)\n",
    "\n",
    "gpg_names_cleaned = []\n",
    "for name in gpg_names_cleaned_0:\n",
    "    if ' limited' in name:\n",
    "        newname = name.replace(' limited', '')\n",
    "        gpg_names_cleaned.append(newname)\n",
    "    else:\n",
    "        gpg_names_cleaned.append(name)\n",
    "                                           \n",
    "# Forming gpg data frame and exporting to csv \n",
    "                         \n",
    "gpg = uk_gpg_all['DiffMeanHourlyPercent']\n",
    "\n",
    "columns = ['name', 'gpg'] \n",
    "uk_gpg = pd.DataFrame(list(zip(gpg_names_cleaned, gpg)), columns = columns)\n",
    "mean_gpg = uk_gpg['DiffMeanHourlyPercent'].mean()\n",
    "uk_gpg['gpg_above_average'] = np.where(uk_gpg_all['DiffMeanHourlyPercent'] > mean_gpg, 'Above', 'Below')\n",
    "uk_gpg = uk_gpg.drop(columns=['gpg'])                                            \n",
    "                                           \n",
    "gpg_data.to_csv('gpg_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
